# 카프카 DLQ(Dead Letter Queue)

## 개요
- DLQ는 처리 실패 메시지를 격리해 정상 스트림을 보호하는 보조 토픽이다.
- 컨슈머가 재시도 끝에 실패하거나 스키마 불일치·비즈니스 검증 오류가 발생했을 때 메시지를 별도 토픽으로 전송한다.
- DLQ 토픽은 일반 토픽과 동일한 파티션 구조를 가지지만, 보존 기간을 더 길게 두거나 compaction을 꺼서 전체 기록을 남기는 편이다.

## 사용 시나리오
1. **스키마 진화 실패**: Avro/JSON 스키마 호환성 문제로 역직렬화가 되지 않을 때 오류 payload와 헤더를 DLQ에 저장한다.
2. **비즈니스 규칙 위반**: 정상 데이터 흐름을 막지 않으면서 문제 데이터를 사후 조사할 수 있다.
3. **다운스트림 장애**: 외부 시스템 장애로 영구 실패가 발생할 경우, DLQ에 보관한 뒤 장애 해결 후 재처리한다.

## 설계 체크리스트
- **전송 책임**: 프로듀서 재시도 실패 시 전송할지, 컨슈머가 실패 메시지를 재발행할지 결정한다.
- **메시지 포맷**: 원본 payload + 메타데이터(오프셋, 파티션, 예외 스택, 처리 시각)를 함께 저장해 디버깅을 쉽게 한다.
- **재처리 파이프라인**: DLQ 토픽을 읽어 수동/자동으로 복구하는 별도 배치 또는 스트림 잡을 마련한다.
- **알림**: DLQ 토픽 메시지 증가 추이, 특정 에러 코드 발생률 등을 모니터링해 즉시 알린다.

## 참조 아키텍처(시각화)
```
Producer ──► Main Topic ──► Consumer Group
                            │
                            ├─ 정상 처리 → Downstream 서비스(결제, ERP 등)
                            └─ 실패 이벤트 ─► DLQ Topic ──► DLQ Processor(Batch/Stream)
                                                        │
                                                        └─ 재처리/장애 알림/대시보드
```
위 구조에서 컨슈머는 실패 시 handler를 통해 DLQ 토픽으로 메시지를 재전송하고, DLQ Processor는 장애 알림과 재처리를 전담한다.

## 실제 운용 예시
1. 주문 서비스는 `order.confirmed` 토픽을 소비한다. 새 필드 `couponId`가 추가되었으나 컨슈머 스키마는 아직 반영되지 않아 역직렬화가 실패한다.
2. 컨슈머는 정해진 횟수(예: 3회)까지 재시도 후 실패로 간주하고, 원본 payload·offset·에러 스택을 JSON 형태로 `order.confirmed.dlq`에 발행한다.
3. 운영자는 DLQ 모니터링에서 급증한 메시지 수를 확인하고, 최근 배포된 주문 서비스에서 스키마 변경이 있었음을 확인한다.
4. 장애 조치 후 DLQ Processor(batch 잡)가 `couponId` 필드를 포함하도록 컨슈머를 업데이트한 뒤, DLQ 메시지를 검증 후 원본 토픽으로 재주입하거나 REST API를 호출해 주문 상태를 보정한다.

## 주의사항
- **무분별한 재시도 금지**: DLQ에 보낼 메시지를 무한 재시도하면 컨슈머 랙이 급증한다. 재시도 횟수와 backoff 정책을 명확히 정의한다.
- **개인정보 노출**: DLQ에는 디버깅 메타데이터가 많이 포함되므로 암호화/마스킹 정책을 그대로 적용해야 한다.
- **파티션 정렬 유지**: 원본 토픽과 동일한 partition key를 사용해야 순서 의존성이 있는 메시지를 재처리할 때 일관성을 보장한다.
- **보존 기간 관리**: 보존 기간이 너무 길면 저장 비용이 증가하고, 너무 짧으면 사후 조사가 불가능하다. 서비스 특성에 맞춰 주기적으로 검토한다.

## 구현 계획
1. **DLQ 명명 규칙과 정책 정의**: `{서비스}.{이벤트}.dlq` 형식, retention 14~30일, compaction off, replication factor 3 등 표준 수립.
2. **실패 처리 공통 모듈화**: 컨슈머/프로듀서 라이브러리에서 실패 핸들러를 공통 지원해 DLQ 메시지 포맷, 전송 책임을 통일한다.
3. **DLQ Processor 구축**: Kafka Streams 또는 Spring Batch로 DLQ 토픽을 주기적으로 스캔해 재처리, Slack/PagerDuty 알림, 장애 티켓 발행을 자동화한다.
4. **모니터링/대시보드**: lag, 메시지 수, 에러 코드 상위 N개 등을 Grafana/Looker 등에 시각화하고 임계치 초과 시 경고한다.
5. **운영 가이드 정리**: 어떤 기준으로 재처리/폐기할지, 승인 절차와 롤백 전략은 무엇인지 운영 문서를 마련한다.
